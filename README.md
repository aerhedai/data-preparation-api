```markdown
# ğŸ“¦ Data Preparation API

**Version:** 1.0.0  
**Author:** Aerhed AI

---

## ğŸ“ Overview

The **Data Preparation API** is a modular microservice for preparing datasets for downstream machine learning tasks.  
It is built using the [Aerhed AI Python API Template](https://github.com/aerhedai/python-api-template) and includes:

- **Scaling**: MinMaxScaler, StandardScaler, RobustScaler, etc.
- **Encoding**: One-hot encoding, label encoding for categorical variables.
- **Splitting**: Train-test split (with optional stratification).
- **Output**: Saves processed dataset and metadata for further processing.

This API is designed to integrate seamlessly with other Aerhed modular APIs in the AI dataset pipeline.

---

## ğŸ“‚ Project Structure

```
data-preparation-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI entry point
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ prepare.py           # API route definitions
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preparation_service.py  # Core preparation logic
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preparation_request.py  # Pydantic models for requests
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ file_handler.py      # File loading/saving helpers
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_preparation.py      # Unit tests
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â””â”€â”€ start.sh
```

---

## ğŸš€ Features

- ğŸ“¤ **Upload** dataset in CSV/Excel format (via shared volume or API call).
- ğŸ›  **Clean & transform**:
  - Encode categorical features
  - Scale numerical features
  - Handle missing values
- âœ‚ **Split** into training/testing sets
- ğŸ“¦ **Save outputs** for downstream APIs

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/aerhedai/data-preparation-api.git
cd data-preparation-api
```

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Environment Variables

Copy `.env.example` to `.env` and update values if necessary:

```bash
cp .env.example .env
```

Example `.env`:

```
HOST=0.0.0.0
PORT=8000
UPLOAD_DIR=uploaded_files
OUTPUT_DIR=prepared_files
```

---

## â–¶ï¸ Running the API

### Locally
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### Docker
```bash
docker build -t data-preparation-api .
docker run -p 8000:8000 data-preparation-api
```

### Docker Compose
If integrating with other APIs:
```bash
docker-compose up --build
```

---

## ğŸ“¡ API Endpoints

### **POST** `/prepare`
Prepares a dataset for ML.

**Request:**
```json
{
  "file_path": "uploaded_files/my_dataset.csv",
  "target_column": "price",
  "scaling": "standard",
  "encoding": "onehot",
  "test_size": 0.2,
  "stratify": true
}
```

**Response:**
```json
{
  "train_file": "prepared_files/train.csv",
  "test_file": "prepared_files/test.csv",
  "scaler": "StandardScaler",
  "encoder": "OneHotEncoder"
}
```

---

## ğŸ§ª Testing

Run unit tests with:

```bash
pytest
```

---

## ğŸ›  Development Notes

- **Preparation logic** lives in `app/services/preparation_service.py`
- **Request validation** in `app/models/preparation_request.py`
- **File handling** in `app/utils/file_handler.py`
- **Routes** in `app/routes/prepare.py`

---

## ğŸ”— Related APIs

- [`dataset-uploader-api`](https://github.com/aerhedai/dataset-uploader-api)
- [`data-profiler-api`](https://github.com/aerhedai/data-profiler-api)
- [`data-cleaner-api`](https://github.com/aerhedai/data-cleaner-api)

---

## ğŸ“œ License
MIT License â€“ see [LICENSE](LICENSE) for details.
```
